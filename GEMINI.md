## Project Overview

This project, `generalized_landcovers` (also known as `OpenLandcoverMap`), is designed to create generalized landcover maps from OpenStreetMap (OSM) data, specifically for zoom levels z0-z8. The standard OSM map can appear empty at these zoom levels, and this project aims to provide a more visually informative map by generalizing landcover features.

The core idea is to use a hexagonal grid (H3) to process OSM data. For each cell in the grid, the project identifies the dominant landcover type based on the area it covers. These cells are then merged to create larger, generalized polygons, which are then styled and rendered into map tiles.

The project uses a combination of technologies:

*   **PostGIS:** A PostgreSQL extension for storing and processing geospatial data.
*   **H3:** A hexagonal hierarchical spatial index.
*   **Mapnik:** A toolkit for rendering maps.
*   **Python:** Used for various scripting and data processing tasks.
*   **Make:** For orchestrating the build and data processing pipeline.

## Building and Running

The project uses a `makefile` to manage the build process. Here are the key commands:

*   `make all`: This is the default command. It runs the entire data processing pipeline, which includes:
    *   Creating tables in the PostGIS database.
    *   Generating shapefiles from the database tables.
    *   Creating a `mapnik.xml` file from the `.mml` and `.mss` style files.
    *   Generating MBTiles for the final map.
*   `make test`: Runs a test script (`test.py`) that likely renders a sample map and compares it to a reference image to ensure the output is correct.
*   `make clean`: Removes generated files and drops the `h3` schema from the PostGIS database.
*   `make import_planet`: Imports a full OpenStreetMap planet file into the PostGIS database. This is a long-running process.
*   `make update_db`: Updates the PostGIS database with the latest changes from OpenStreetMap.

## Development Conventions

*   **Build Process:** The entire build and data processing pipeline is managed by the `makefile`.
*   **Data Processing:** The core generalization logic is implemented in SQL scripts, which are located in the `sql-scripts/` directory. These scripts are executed by `psql` commands in the `makefile`.
*   **Styling:** The map's visual appearance is defined in `.mss` (Map Style Sheet) files. These are compiled into a `mapnik.xml` file using the `carto` tool.
*   **Map Layers:** The `project.mml` file defines the layers of the map, referencing the shapefiles generated by the data processing pipeline.
*   **Scripting:** Python scripts are used for various helper tasks, such as generating taginfo JSON and creating HTML pages.


## Next Steps

Prioritized via MoSCoW method:

### Must
*  Complete the Empty Hex Viewer feature. We need to decouple it from the production postgress DB and create SQLite snapshot of the necessary tables
(see the 'Plan: Migrate Frontend API to SQLite' section below).

* Rewrite the [README.md](README.md). It looks quite obsolete and pointless in comparison with the [About page](webui-prototypes/about.html)

### Should
*  Integrate the Gantt-chart view feature (`generate_gantt_chart.py`) into the process feature (probably into `run.sh`)
*  Use common style for the Gantt chart page, the same as for other pages.
*  **Migrate to vector tiles** (See the 'Plan: Modernize the Rendering Stack' secton below)

### Could
None

### Would Not
None



## Plan: Modernize the Rendering Stack

The goal is to enhance the interactive web map by switching to vector tiles, while continuing to provide Shapefiles as a primary download artifact. This plan outlines a dual-output strategy.

### Dual-Artifact Approach

The project will produce two sets of outputs:
1.  **Shapefiles (`.shp`):** For download and use in desktop GIS software. The generation process for these remains unchanged.
2.  **Vector Tiles (`.mvt`):** For a fast and interactive web map. This is a new addition to the workflow.

### High-Level Migration Plan

1.  **Preserve Shapefile Generation:** The existing targets in the `makefile` that use `ogr2ogr` to create Shapefiles from PostGIS tables will be kept as is. The process of creating downloadable `.zip` archives will also remain.

2.  **Add Vector Tile Generation:**
    *   New targets will be added to the `makefile`.
    *   These targets will read from the same final PostGIS tables (e.g., `h3.landcovers_aggr`, `h3.places`).
    *   They will use PostGIS's built-in `ST_AsMVT()` function to generate Mapbox Vector Tiles (`.mvt`) directly from the database.
    *   The output tiles will be stored in a separate directory, such as `data/tiles/{z}/{x}/{y}.mvt`.

3.  **Implement Client-Side Rendering:**
    *   A lightweight tile-server can be set up to serve the newly generated `.mvt` files.
    *   The web map interface will be updated to use a client-side rendering library like **MapLibre GL JS**.
    *   A new JSON-based style file will be created to define the map's appearance, replacing the need for `.mss` files for the web map.

4.  **Update Build Workflow:**
    *   The parts of the `makefile` responsible for rendering **raster tiles for the web map** (i.e., using `carto` and `node ../tilemill/index.js` to create PNGs/MBTiles) can be deprecated and removed.
    *   The Shapefile generation workflow remains fully intact.

This approach ensures that the project's existing functionality is preserved while adding the significant benefits of a modern, vector-tile-based web map.

## Plan: Migrate Frontend API to SQLite

The goal is to transition the frontend API scripts (`misc/empty_hex_api.py` and `misc/country_api.py`) from directly accessing the PostgreSQL database to reading data from a pre-generated SQLite file. This will decouple the frontend from the main PostGIS database.

### High-Level Migration Plan

1.  **Create Python Script for SQLite Export (`py-scripts/export_stats_to_sqlite.py`):**
    *   This script will connect to the PostgreSQL database.
    *   It will query the `h3.no_landcover_per_country` and `h3.country_stats` tables.
    *   It will then create an SQLite database file (e.g., `data/export/landcover_stats.sqlite`).
    *   For each queried table, it will create a corresponding table in the SQLite database and insert the data.

2.  **Integrate SQLite Export into `makefile`:**
    *   Add a new target to the `makefile` (e.g., `export_sqlite_stats`).
    *   This target will execute the Python script created in step 1.
    *   Ensure this target is part of the main build process (e.g., `all` target) so the SQLite file is always up-to-date.

3.  **Modify API Scripts (`misc/empty_hex_api.py` and `misc/country_api.py`):**
    *   Update these scripts to connect to the `data/export/landcover_stats.sqlite` file instead of the PostgreSQL database.
    *   Rewrite the database query logic within these scripts to use SQLite syntax. This includes adapting connection strings, cursor handling, and SQL queries as necessary.

This plan ensures that the frontend API can operate independently of the PostGIS database, relying on a static SQLite snapshot of the required data.

## Plan: Generate Gantt Chart for Makefile

The goal is to create a web page that visualizes the `makefile` build process as a Gantt chart. This page will be generated by a Python script as part of the build process.

### High-Level Migration Plan

1.  **Create Python Script (`py-scripts/generate_gantt_chart.py`):**
    *   **Purpose:** Parse `makefile` and `makefile-profiling.log` to generate an HTML page visualizing the build process as a Gantt chart.
    *   **Inputs:**
        *   `makefile`: The project's `makefile` to extract task names and dependencies.
        *   `makefile-profiling.log`: A log file containing task execution durations. (The third field contains duration in milliseconds, which will be converted to seconds).
    *   **Output:**
        *   `data/export/gantt_chart.html`: An HTML file containing the interactive Gantt chart.

2.  **Implementation Details for `py-scripts/generate_gantt_chart.py`:**
    *   **Read `makefile-profiling.log`:**
        *   Open and read `makefile-profiling.log`.
        *   For each line, extract the task name and its duration (the third field). **Convert the duration from milliseconds to seconds.** Store this in a dictionary: `{'task_name': duration_in_seconds}`.
    *   **Parse `makefile`:**
        *   Use or adapt parsing logic from `../make-profiler` to read the `makefile` and build a dependency graph between targets.
        *   Construct a dependency graph where nodes are targets and edges represent dependencies.
    *   **Calculate Task Timings:**
        *   Perform a topological sort on the dependency graph to determine the order of execution.
        *   For each task, calculate its start and end times based on its duration (in seconds, from `makefile-profiling.log`) and the completion times of its prerequisites.
    *   **Generate HTML/CSS for Gantt Chart:**
        *   Generate an HTML file that will contain:
            *   Basic HTML structure.
            *   Embedded CSS styles for rendering Gantt bars (each bar represents a task, its length proportional to duration, and position to start time).
            *   Minimal JavaScript might be used for dynamic positioning or scaling if required for visualization.
            *   Task data (name, start time, duration) will be embedded directly into the HTML structure (e.g., as `data-` attributes or generated `div` elements).

3.  **Integrate into `makefile`:**
    *   Add a new target:
        ```makefile
        data/export/gantt_chart.html: py-scripts/generate_gantt_chart.py makefile makefile-profiling.log | data/export
            python3 $<
        ```
    *   Add `data/export/gantt_chart.html` to the `all` target or a new `profile` target to ensure it's generated during the build process.

This plan ensures that the Gantt chart visualization is generated as part of the build process, providing insights into task durations and dependencies.
- Added `waterbodies.zip` to the downloads page. This involved creating a `misc/waterbodies.readme.txt` file, modifying the `makefile` to generate `data/export/downloads/waterbodies.zip` and include it as a dependency for `downloads.html`, and updating `py-scripts/downloads.py` to list the new zip file in the generated HTML.
